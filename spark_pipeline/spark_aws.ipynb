{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11238ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_ACCESS_KEY = \"AK...\"\n",
    "AWS_SECRET_KEY = \"dC...\"\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", AWS_ACCESS_KEY)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", AWS_SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d0e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "games_df = spark.read.csv(\"s3://spark_pipeline/csv/games-expand.csv\", header=True, inferSchema = True)\n",
    "display(games_df)\n",
    "\n",
    "stats_df = spark.read.csv(\"s3://spark_pipeline/csv/game_skater_stats.csv\", header=True, inferSchema=True)\n",
    "display(stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4778cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Avro, Parquet, ORC, csv\n",
    "# Can convert between one another\n",
    "\n",
    "\n",
    "# AVRO write\n",
    "avro_path = \"s3://spark_pipeline/avro/game_skater_stats/\"\n",
    "stats_df.write.mode('overwrite').format(\"com.databricks.spark.avro\").save(avro_path)\n",
    "\n",
    "# AVRO read \n",
    "avro_df = sqlContext.read.format(\"com.databricks.spark.avro\").load(avro_path)\n",
    "\n",
    "# parquet out\n",
    "parquet_path = \"s3a://spark_pipeline/games-parquet/\"\n",
    "avro_df.write.mode('overwrite').parquet(parquet_path)\n",
    "\n",
    "# parquet in\n",
    "parquet_df = sqlContext.read.parquet(parquet_path)\n",
    "\n",
    "# orc out\n",
    "orc_path = \"s3a://spark_pipeline/games-orc/\"\n",
    "parquet_df.write.mode('overwrite').orc(orc_path)\n",
    "\n",
    "# orc in\n",
    "orc_df = sqlContext.read.orc(orc_path)\n",
    "\n",
    "# CSV out\n",
    "csv_path = \"s3a://spark_pipeline/games-csv-out/\"\n",
    "orc_df.coalesce(1).write.mode('overwrite').format(\n",
    " \"com.databricks.spark.csv\").option(\"header\",\"true\").save(csv_path)\n",
    "  \n",
    "# and CSV read to finish the round trip \n",
    "csv_df = spark.read.csv(csv_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36633fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Koalas\n",
    "import databricks.koalas as ks\n",
    "\n",
    "stats_ks = stats_df.to_koalas()\n",
    "stats_df = stats_ks.to_spark()\n",
    "\n",
    "print(stats_ks['timeOnIce'].mean())\n",
    "print(stats_ks.iloc[:1, 1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78541941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark SQL\n",
    "new_df = spark.sql(\"\"\"\n",
    "  select player_id, sum(1) as games, sum(goals) as goals\n",
    "  from stats\n",
    "  group by 1\n",
    "  order by 3 desc\n",
    "  limit 5\n",
    "\"\"\")\n",
    "\n",
    "display(new_df)\n",
    "\n",
    "\n",
    "display(spark.sql(\"\"\"\n",
    "  select cast(goals/shots * 50 as int)/50.0 as Goals_per_shot\n",
    "      ,sum(1) as Players \n",
    "  from (\n",
    "    select player_id, sum(shots) as shots, sum(goals) as goals\n",
    "    from stats\n",
    "    group by 1\n",
    "    having goals >= 5\n",
    "  )  \n",
    "  group by 1\n",
    "  order by 1\n",
    "\"\"\"))\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# dropping columns\n",
    "copy_df = stats_df.drop('game_id', 'player_id')\n",
    "\n",
    "# selection columns \n",
    "copy_df = copy_df.select('assists', 'goals', 'shots')\n",
    "\n",
    "# adding columns\n",
    "copy_df = copy_df.withColumn(\"league\", lit('NHL'))\n",
    "display(copy_df)\n",
    "\n",
    "copy_df = stats_df.select('game_id', 'player_id').withColumn(\"league\", lit('NHL'))\n",
    "df = copy_df.join(stats_df, ['game_id', 'player_id'])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d67198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "\n",
    "# Create the schema for the resulting dataframe\n",
    "schema = StructType([StructField('ID', LongType(), True),\n",
    "                     StructField('p0', DoubleType(), True),\n",
    "                     StructField('p1', DoubleType(), True)])\n",
    "\n",
    "# Define the UDF, input and outputs are Pandas DFs\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def analyze_player(sample_pd):\n",
    "    \n",
    "    # return empty params in not enough data\n",
    "    if (len(sample_pd.shots) <= 1):\n",
    "        return pd.DataFrame({'ID': [sample_pd.player_id[0]], \n",
    "                                   'p0': [ 0 ], 'p1': [ 0 ]})\n",
    "     \n",
    "    # Perform curve fitting     \n",
    "    result = leastsq(fit, [1, 0], args=(sample_pd.shots, \n",
    "                                  sample_pd.hits))\n",
    "    \n",
    "    # Return the parameters as a Pandas DF \n",
    "    return pd.DataFrame({'ID': [sample_pd.player_id[0]], \n",
    "                       'p0': [result[0][0]], 'p1': [result[0][1]]})\n",
    "    \n",
    "# perform the UDF and show the results \n",
    "player_df = stats_df.groupby('player_id').apply(analyze_player)\n",
    "display(player_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639be3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLlib\n",
    "games_df.createOrReplaceTempView(\"games_df\")\n",
    "\n",
    "games_df = spark.sql(\"\"\"\n",
    "  select *, row_number() over (order by rand()) as user_id\n",
    "    ,case when rand() > 0.7 then 1 else 0 end as test\n",
    "  from games_df\n",
    "\"\"\")\n",
    "trainDF = games_df.filter(\"test == 0\")\n",
    "testDF = games_df.filter(\"test == 1\")\n",
    "print(\"Train \" + str(trainDF.count()))\n",
    "print(\"Test \" + str(testDF.count()))\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# create a vector representation\n",
    "assembler = VectorAssembler(inputCols=trainDF.schema.names[0:10], outputCol=\"features\")\n",
    "\n",
    "trainVec = assembler.transform(trainDF).select('label', 'features')\n",
    "testVec = assembler.transform(testDF).select('label', 'features', 'user_id')\n",
    "display(testVec)\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# specify the columns for the model\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "# fit on training data\n",
    "model = lr.fit(trainVec)\n",
    "\n",
    "# predict on test data \n",
    "predDF = model.transform(testVec)\n",
    "display(predDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4140bea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "roc = BinaryClassificationEvaluator().evaluate(predDF)\n",
    "print(roc)\n",
    "\n",
    "# Retrieving the results\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# split out the array into a column \n",
    "secondElement = udf(lambda v:float(v[1]),FloatType())\n",
    "predDF = predDF.select(\"*\", secondElement(\"probability\").alias(\"propensity\"))\n",
    "display(predDF)\n",
    "\n",
    "# save results to S3\n",
    "results_df = predDF.select(\"user_id\", \"propensity\")\n",
    "results_path = \"s3a://spark_pipeline/game-predictions/\"\n",
    "results_df.write.mode('overwrite').parquet(results_path)\n",
    "\n",
    "# plot the predictions \n",
    "predDF.createOrReplaceTempView(\"predDF\")\n",
    "\n",
    "plotDF = spark.sql(\"\"\"\n",
    "  select cast(propensity*100 as int)/100 as propensity, \n",
    "         label, sum(1) as users\n",
    "  from predDF \n",
    "  group by 1, 2\n",
    "  order by 1, 2  \n",
    "\"\"\")\n",
    "\n",
    "# table output\n",
    "display(plotDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a38b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning model\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import models, layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10,)))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "history = model.fit(x_train, y_train, epochs=100, batch_size=100, validation_split = .2, verbose=0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "fig = plt.figure(figsize=(10,6) )\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6229f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF partition and model application\n",
    "testDF.createOrReplaceTempView(\"testDF \")\n",
    "\n",
    "partitionedDF = spark.sql(\"\"\"\n",
    "  select *, cast(rand()*100 as int) as partition_id\n",
    "  from testDF \n",
    "\"\"\")\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([StructField('user_id', LongType(), True),\n",
    "                     StructField('propensity', DoubleType(),True)])\n",
    "\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def apply_keras(pd):\n",
    "    pd['propensity'] = model.predict(pd.iloc[:,0:10])\n",
    "    return pd[['user_id', 'propensity']]\n",
    "\n",
    "results_df=partitionedDF.groupby('partition_id').apply(apply_keras)\n",
    "display(results_df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd175d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated feature engineering\n",
    "plays_df = spark.read.csv(\"s3://spark_pipeline/csv/game_plays.csv\", \n",
    "            header=True, inferSchema = True).drop(\n",
    "            'secondaryType', 'periodType', 'dateTime', 'rink_side')\n",
    "plays_pd = plays_df.filter(\"rand() < 0.003\").toPandas()\n",
    "plays_pd.shape\n",
    "# pip3 install featuretools\n",
    "import featuretools as ft\n",
    "from featuretools import Feature\n",
    "\n",
    "es = ft.EntitySet(id=\"plays\")\n",
    "es = es.entity_from_dataframe(entity_id=\"plays\",dataframe=plays_pd,\n",
    "                    index=\"player_id\", variable_types = {\n",
    "                    \"event\": ft.variable_types.Categorical,\n",
    "                    \"description\": ft.variable_types.Categorical })\n",
    "\n",
    "\n",
    "f1 = Feature(es[\"plays\"][\"event\"])\n",
    "f2 = Feature(es[\"plays\"][\"description\"])\n",
    "\n",
    "encoded, defs = ft.encode_features(plays_pd, [f1, f2], top_n=10)\n",
    "encoded.reset_index(inplace=True)\n",
    "\n",
    "es = ft.EntitySet(id=\"plays\")\n",
    "es = es.entity_from_dataframe(entity_id=\"plays\", \n",
    "                                dataframe=encoded, index=\"play_id\")\n",
    "\n",
    "es = es.normalize_entity(base_entity_id=\"plays\", \n",
    "                            new_entity_id=\"games\", index=\"game_id\")\n",
    "features, transform=ft.dfs(entityset=es, \n",
    "                                 target_entity=\"games\",max_depth=2)\n",
    "features.reset_index(inplace=True)\n",
    "\n",
    "features.columns = features.columns.str.replace(\"[(). =]\", \"\")\n",
    "schema = sqlContext.createDataFrame(features).schema\n",
    "features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46505c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket IDs \n",
    "plays_df.createOrReplaceTempView(\"plays_df\")\n",
    "plays_df = spark.sql(\"\"\"\n",
    "  select *, abs(hash(game_id))%1000 as partition_id \n",
    "  from plays_df \n",
    "\"\"\")\n",
    "\n",
    "# Full feature transformation\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def gen_features(plays_pd):\n",
    "\n",
    "    es = ft.EntitySet(id=\"plays\")\n",
    "    es = es.entity_from_dataframe(entity_id=\"plays\",dataframe=plays_pd,\n",
    "                    index=\"player_id\", variable_types = {\n",
    "                    \"event\": ft.variable_types.Categorical,\n",
    "                    \"description\": ft.variable_types.Categorical })\n",
    "                    \n",
    "    encoded_features = ft.calculate_feature_matrix(defs, es)    \n",
    "    encoded_features.reset_index(inplace=True)\n",
    "  \n",
    "    es = ft.EntitySet(id=\"plays\")\n",
    "    es = es.entity_from_dataframe(entity_id=\"plays\", \n",
    "                                dataframe=encoded, index=\"play_id\")\n",
    "    es = es.normalize_entity(base_entity_id=\"plays\",\n",
    "                            new_entity_id=\"games\", index=\"game_id\")\n",
    "    generated = ft.calculate_feature_matrix(transform,es).fillna(0)\n",
    "    \n",
    "    generated.reset_index(inplace=True)\n",
    "    generated.columns = generated.columns.str.replace(\"[(). =]\",\"\")\n",
    "    return generated \n",
    "  \n",
    "features_df = plays_df.groupby('partition_id').apply(gen_features)\n",
    "display(features_df)      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
